# -*- coding: utf-8 -*-
"""Satellite Image Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1obU58yjV1qgNYuxsWkhTpuOL0Ku5aax6
"""

from google.colab import drive
drive.mount('/content/drive')

import os

# Provide the source folder
src_folder = "/content/drive/MyDrive/AIClub_AP_Pentela_Lakshmi_Rishi/Project/Datasets/Train"
# Provide the destination folder
dest_folder = "/content/drive/MyDrive/AIClub_AP_Pentela_Lakshmi_Rishi/Project/Datasets/Test"

# Find all subfolders in the source folder src_folder
directory_contents = os.listdir(src_folder)
print('directory_contents: ', directory_contents)

# Append the path of the src_folder with the sub-folders
sub_folders = []
for item in directory_contents:
  if os.path.isdir(os.path.join(src_folder,item)):
    sub_folders.append(item)
print('sub_folders: ', sub_folders)

import os

# Create the same folder structure in destination
for folder_name in sub_folders:
  # Check if the directory already exists
  if not os.path.exists(os.path.join(dest_folder,folder_name)):
    os.mkdir(os.path.join(dest_folder,folder_name)) # Create the directory only if it doesn't exist

# List full paths of all images in each category/sub-folder
list_files = []
# Find all the files, note that you have to change it to tif if that is the extension
for folder_name in directory_contents:
  files_list = [_ for _ in os.listdir(os.path.join(src_folder, folder_name))]
  # Create the list with full path
  full_path_files_list = []
  for file_name in files_list:
    full_path_files_list.append(os.path.join(src_folder, folder_name, file_name))
  list_files.append(full_path_files_list)
print(list_files[1])

import random

# Specify the fraction
#this is the splitting percentage. 10% will be sent to validation
x = 20

for files_list in list_files:
  # length of the list
  list_len = len(files_list)
  # length of x%
  frac_len = int(list_len * (x/100))
  # select x% of files
  files_to_move = random.sample(files_list, frac_len)
  for single_file in files_to_move:
    # extract the sub-string with subfolder/category and file name
    subfolder_file = '/'.join(single_file.split('/')[-2:])
    # destination path
    dst_folder = os.path.join(dest_folder, subfolder_file)
    # Move the file
    os.replace(single_file, dst_folder)

for folder_name in sub_folders:
  print(folder_name, len(os.listdir(os.path.join(dest_folder,folder_name))))

import os

# Provide the source folder
src_folder = "/content/drive/MyDrive/AIClub_AP_Pentela_Lakshmi_Rishi/Project/Datasets/Train"
# Provide the destination folder
dest_folder = "/content/drive/MyDrive/AIClub_AP_Pentela_Lakshmi_Rishi/Project/Datasets/Valid"

# Find all subfolders in the source folder src_folder
directory_contents = os.listdir(src_folder)
print('directory_contents: ', directory_contents)

# Append the path of the src_folder with the sub-folders
sub_folders = []
for item in directory_contents:
  if os.path.isdir(os.path.join(src_folder,item)):
    sub_folders.append(item)
print('sub_folders: ', sub_folders)

import os

# Create the same folder structure in destination
for folder_name in sub_folders:
  # Check if the directory already exists
  if not os.path.exists(os.path.join(dest_folder,folder_name)):
    os.mkdir(os.path.join(dest_folder,folder_name)) # Create the directory only if it doesn't exist

# List full paths of all images in each category/sub-folder
list_files = []
# Find all the files, note that you have to change it to tif if that is the extension
for folder_name in directory_contents:
  files_list = [_ for _ in os.listdir(os.path.join(src_folder, folder_name))]
  # Create the list with full path
  full_path_files_list = []
  for file_name in files_list:
    full_path_files_list.append(os.path.join(src_folder, folder_name, file_name))
  list_files.append(full_path_files_list)
print(list_files[1])

import random

# Specify the fraction
#this is the splitting percentage. 10% will be sent to validation
x = 20

for files_list in list_files:
  # length of the list
  list_len = len(files_list)
  # length of x%
  frac_len = int(list_len * (x/100))
  # select x% of files
  files_to_move = random.sample(files_list, frac_len)
  for single_file in files_to_move:
    # extract the sub-string with subfolder/category and file name
    subfolder_file = '/'.join(single_file.split('/')[-2:])
    # destination path
    dst_folder = os.path.join(dest_folder, subfolder_file)
    # Move the file
    os.replace(single_file, dst_folder)

for folder_name in sub_folders:
  print(folder_name, len(os.listdir(os.path.join(dest_folder,folder_name))))

# Please replace the brackets below with the drive location of your folders which included subfolders for images
# Sample path: /content/drive/My Drive/ImageClassification
TRAINING_PATH = '/content/drive/MyDrive/AIClub_AP_Pentela_Lakshmi_Rishi/Project/Datasets/Train'
VALIDATION_PATH = '/content/drive/MyDrive/AIClub_AP_Pentela_Lakshmi_Rishi/Project/Datasets/Valid'

def create_model(base_model, num_classes):
    import tensorflow as tf
    # Grab the last layer and add a few extra layers to it
    x=base_model.output
    x=GlobalAveragePooling2D()(x)
    # Dense layer 1
    x=tf.keras.layers.Dense(100,activation='relu', kernel_initializer=tf.keras.initializers.VarianceScaling(), use_bias=True)(x)

    # Final layer with softmax activation
    preds=tf.keras.layers.Dense(num_classes,activation='softmax', kernel_initializer=tf.keras.initializers.VarianceScaling(), use_bias=False)(x)

    # Create the final model
    model=Model(inputs=base_model.input,outputs=preds)
    return model

def get_optimizer(optimizer_name, learning_rate):
    # Import keras optimizers
    from tensorflow.keras.optimizers import Adam, Adadelta, Adagrad, Adamax, Ftrl, Nadam, RMSprop, SGD
    print('Selected Optimizer', optimizer_name)
    switcher = {
        'Adadelta': Adadelta(learning_rate=learning_rate),
        'Adagrad': Adagrad(learning_rate=learning_rate),
        'Adam': Adam(learning_rate=learning_rate),
        'Adamax': Adamax(learning_rate=learning_rate),
        'FTRL': Ftrl(learning_rate=learning_rate),
        'NAdam': Nadam(learning_rate=learning_rate),
        'RMSprop': RMSprop(learning_rate=learning_rate),
        'Gradient Descent': SGD(learning_rate=learning_rate)
    }
    # If optimizer_name is empty, Adam will be return as default optimizer
    return switcher.get(optimizer_name, Adam(learning_rate=learning_rate))

import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf

from keras.applications.mobilenet_v2 import preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dense, GlobalAveragePooling2D
from keras.models import Model
from tensorflow.keras import regularizers
from tensorflow.keras.preprocessing import image_dataset_from_directory
from keras.callbacks import EarlyStopping
from tensorflow import keras

epochs = 20
base_learning_rate = 0.001
optimizer = 'Adam'
BATCH_SIZE = 16
IMG_SIZE = (224, 224)

train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

train_generator = train_datagen.flow_from_directory(TRAINING_PATH,
                                                   target_size=IMG_SIZE,
                                                   color_mode='rgb',
                                                   batch_size=BATCH_SIZE,
                                                   class_mode='categorical',
                                                   shuffle=True)

validation_generator = validation_datagen.flow_from_directory(VALIDATION_PATH,
                                                             target_size=IMG_SIZE,
                                                             color_mode='rgb',
                                                             batch_size=BATCH_SIZE,
                                                             class_mode='categorical',
                                                             shuffle=True)

base_model = keras.applications.ResNet50(
    include_top=True,
    weights="imagenet",
    input_shape=(224, 224, 3)
)

for layer in base_model.layers:
    layer.trainable=False

#base_model = keras.applications.ResNet50(
 #   include_top=False,  # Remove the top classification layers
  #  weights="imagenet",
   # input_shape=(224, 224, 3)
#)
model = create_model(base_model, len(train_generator.class_indices))
model.compile(optimizer=get_optimizer(optimizer, base_learning_rate),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history_fine = model.fit(train_generator,
                         epochs=epochs,
                         validation_data=validation_generator,
                         verbose=1)
early_stopping_monitor = EarlyStopping(
    monitor='val_loss',
    min_delta=0,
    patience=30,
    verbose=0,
    mode='auto',
    baseline=None,
    restore_best_weights=True
)

# Import necessary libraries
import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf

from keras.applications.mobilenet_v2 import preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dense, GlobalAveragePooling2D
from keras.models import Model
from keras.callbacks import EarlyStopping

# Initialize hyperparameters
epochs = 20
base_learning_rate = 0.05
optimizer = 'Adam'
BATCH_SIZE = 32
IMG_SIZE = (224, 224)

# Create data generators for training and validation
train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

train_generator = train_datagen.flow_from_directory(TRAINING_PATH,
                                                   target_size=IMG_SIZE,
                                                   color_mode='rgb',
                                                   batch_size=BATCH_SIZE,
                                                   class_mode='categorical',
                                                   shuffle=True)

validation_generator = validation_datagen.flow_from_directory(VALIDATION_PATH,
                                                             target_size=IMG_SIZE,
                                                             color_mode='rgb',
                                                             batch_size=BATCH_SIZE,
                                                             class_mode='categorical',
                                                             shuffle=True)

# Download the MobileNetV2 model with specified parameters
base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet', alpha=0.35)

# Freeze the layers of the base model
for layer in base_model.layers:
    layer.trainable = False

# **Determine the number of classes from the training data**
num_classes = len(train_generator.class_indices)
print("Number of classes found: ", num_classes) # Add this line to verify

# Create the top layers for classification
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)  # Add more dense layers as needed
predictions = Dense(num_classes, activation='softmax')(x) # Update num_classes here

# Combine base model and top layers into a new model
model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model with optimizer, loss function, and metrics
model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Define early stopping to prevent overfitting
early_stopping_monitor = EarlyStopping(
    monitor='val_loss',
    min_delta=0,
    patience=30,
    verbose=0,
    mode='auto',
    restore_best_weights=True
)

# Train the model
history_fine = model.fit(train_generator,
                         epochs=epochs,
                         validation_data=validation_generator,
                         callbacks=[early_stopping_monitor],
                         verbose=1)

import os

MODEL_PATH = "/content/drive/MyDrive/AIClub_AP_Pentela_Lakshmi_Rishi/Project/Best Model"
MODEL_NAME = "satellite_image_classification_model_30_0.001.h5"
model.save(os.path.join(MODEL_PATH, MODEL_NAME))

def visualization():
    import pandas as pd
    df = pd.DataFrame(history_fine.history)
    #loss plots
    plt.figure(figsize=(8,8))
    plt.plot(df['loss'], color='red', label = "Training_loss")
    plt.plot(df['val_loss'], color='blue')
    plt.legend(['Training Loss','Validation loss'],loc = 'best' )
    plt.title('Line plot of Training and Validation loss')
    plt.ylim(0,1)
    plt.show()

    #accuracy plots
    plt.figure(figsize=(8,8))
    plt.plot(df['accuracy'], color='red')
    plt.plot(df['val_accuracy'], color='blue')
    plt.legend(['Training acc','Validation acc'],loc = 'best' )
    plt.title('Line plot of Training and Validation Accuracies')
    plt.ylim(0,1)
    plt.show()

visualization()

# Import numpy for calculating best model accuracy
import numpy as np
# Populating matrics -> accuracy & loss
acc = history_fine.history['accuracy']
val_acc = history_fine.history['val_accuracy']

loss = history_fine.history['loss']
val_loss = history_fine.history['val_loss']

print('Training Accuracy: ', acc)
print('Validation Accuracy: ', val_acc)
print('Training Loss: ', loss)
print('Validation Loss: ', val_loss)
best_model_accuracy = history_fine.history['val_accuracy'][np.argmin(history_fine.history['val_loss'])]
print('best model accuracy: ', best_model_accuracy)

def seperate_labels(generator):
    x_validation = []
    y_validation = []
    num_seen = 0

    for x, labels in generator:
        x_validation.append(x)
        y_validation.append([argmax(label) for label in labels])
        num_seen += len(x)
        if num_seen == generator.n: break

    x_validation = np.concatenate(x_validation)
    y_validation = np.concatenate(y_validation)
    return x_validation, y_validation

# Calculate and display the confusion matrix
import matplotlib.pyplot as plt
from numpy.core.fromnumeric import argmax
from sklearn.metrics import ConfusionMatrixDisplay

x_validation, y_validation = seperate_labels(validation_generator)
y_pred = model.predict(x_validation, batch_size=BATCH_SIZE)
predictions = np.apply_along_axis(argmax, 1, y_pred)
display_labels = validation_generator.class_indices.keys()

# ConfusionMatrixDisplay.from_predictions(y_validation, predictions, display_labels=display_labels, cmap="binary")
# plt.show()

import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
plt.figure(figsize = (5,5))
sns.heatmap(confusion_matrix(y_validation, predictions), annot = True, fmt = 'g', cmap = "Blues",xticklabels=display_labels, yticklabels=display_labels)
plt.title("Confusion Matrix")
plt.show()

print(classification_report(y_validation, predictions))

# Predicting code for an image
from tensorflow.keras.preprocessing import image
# Please replace the brackets below with the location of your image which need to predict
img_path = '/content/drive/MyDrive/AIClub_AP_Pentela_Lakshmi_Rishi/Project/Datasets/Valid/forest/Forest_2898.jpg'
img = image.load_img(img_path, target_size=IMG_SIZE)
img_array = image.img_to_array(img)
img_batch = np.expand_dims(img_array, axis=0)
img_preprocessed = preprocess_input(img_batch)
prediction = model.predict(img_preprocessed)
print(prediction)